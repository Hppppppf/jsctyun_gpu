---
title: "模型部署手册（英伟达）"
meta_title: "模型部署手册（英伟达）"
description: "英伟达部署DeepSeek"
date: 2025-03-28T08:00:00Z
categories: ["部署手册"]
author: "武永强"
tags: ["英伟达", "部署"]
draft: false
---

本文将介绍如何在英伟达设备部署各参数量模型，为方便大家快速了解在英伟达设备上部署模型的过程，将首先介绍英伟达部署架构及层级关系，之后将以DeepSeek系列模型部署为例，分别介绍单节点部署与多节点部署操作步骤，手册中涉及的docker镜像可在云省分云盘中获取或至官网拉取。
<!--more-->


## 整体架构

![image.png](/images/架构图.png)

## 机器环境整备

### 配置估算

- 根据计划部署的模型，估算机器配置，主要为GPU数量与存储空间

```bash
# 模型显存测算
模型显存占用(GB) = 模型参数量 * 单个参数占用字节数 / 10^9 * 1.2. 
 -- 模型参数量：7B=70亿个参数，671B=6710亿个参数
 -- 单个参数占用字节数：Int4精度=0.5Byte，FP8精度/Int8精度=1Byte，FP16/BF16=2Byte
 -- 10^9：Byte到GB转化，Byte->KB->MB->GB
 -- 1.2：kv cache等中间占用损耗系数

# 模型显存测算举例
DeepSeek 671B满血版模型在FP8下显存占用估算：
显存测算如下：6710 * 10^8 * 1Byte / 10^9 * 1.2 = 805GB
机器台数测算：以英伟达H20 96G机器为例，单台显存为8*96=768GB，805GB/768GB=1.1台
所以需要2台H20服务器进行部署
```

### 存储整备

- 根据模型权重大小，准备机器存储

```bash
# 磁盘分区挂载
## 1、查看设备磁盘情况与文件系统类型
lsblk -f 

## 2、对磁盘进行分区【如无需分区，可跳过此步骤】
parted /dev/nvme1n1 # 要分区的磁盘
p # 查看磁盘当前分区情况
mklabel gpt # 将磁盘格式转化为gpt
mkpart nvme1n1p1 # 确定分区名字，交互式确定文件类型（ext4)，分区大小等

## 3、格式化
mkfs -t ext4/xfs /dev/nvme1n1p1 # 格式化分区，能xfs选xfs，不能ext4

## 4、挂载分区
mkdir /root/test_file # 创建路径
mount /dev/nvme1n1p1 /root/test_file # 挂载路径

## 5、持久化
blkid /dev/nvme1n1p1 # 查看磁盘分区uuid
echo 'UUID="1463fdcf-7360-4eb2-a671-d07262266121"  /root/test_file  \
ext4  defaults 0  0' >> /etc/fstab # 修改/etc/fstab

“”“
第一列：设备文件或UUID或label（三者的区别看下面）
第二列：设备的挂载点（空目录）
第三列：该分区文件系统的格式（可以使用特殊的参数auto，自动识别分区的分区格式）
第四列：文件系统的参数，设置格式的选项
第五列：dump备份的设置（0表示不进行dump备份，1代表每天进行dump备份，2代表不定日期的进行dump备份）
第六列：磁盘检查设置（其实是一个检查顺序，0代表不检查，1代表第一个检查，2后续.一般根目录是1，数字相同则同时检查
”“”
```

### Docker安装

- 首先通过docker -v确认机器是否有docker，如有请忽略此章节，如无可参考此章节安装。
- Docker安装包下载地址：[https://download.docker.com/linux/ubuntu/dists/](https://download.docker.com/linux/ubuntu/dists/)

```bash
# containerd.io_<version>_<arch>.deb
# docker-ce_<version>_<arch>.deb
# docker-ce-cli_<version>_<arch>.deb
# docker-buildx-plugin_<version>_<arch>.deb
# docker-compose-plugin_<version>_<arch>.deb

# 确认操作系统版本
cat /etc/os-release

# 根据操作系统，获取安装包,以ubuntu为例
wget https://download.docker.com/linux/ubuntu/dists/xxxxxxx.deb

# 安装
sudo dpkg -i ./containerd.io_<version>_<arch>.deb \
  ./docker-ce_<version>_<arch>.deb \
  ./docker-ce-cli_<version>_<arch>.deb \
  ./docker-buildx-plugin_<version>_<arch>.deb \
  ./docker-compose-plugin_<version>_<arch>.deb

# 启动docker 
sudo systemctl start docker
# 验证安装
docker info
  
  
# 配置国内源
sudo vim /etc/docker/daemon.json
# 添加源
{
    "registry-mirrors": [
        "https://registry.docker-cn.com",
        "https://docker.mirrors.ustc.edu.cn",
        "https://hub-mirror.c.163.com",
        "https://mirror.baidubce.com",
        "https://ccr.ccs.tencentyun.com"
    ]
}

# 重启docker
sudo systemctl daemon-reload		#重启daemon进程
sudo systemctl restart docker		#重启docker

# 将用户添加至docker组
cat /etc/group
sudo gpasswd -a $用户名 docker
logout
## 如果无docker组
sudo groupadd docker
sudo gpasswd -a $用户名 docker
```

### Miniconda安装

- 开发环境包管理工具下载与安装
- 下载地址：[https://repo.anaconda.com/miniconda/](https://repo.anaconda.com/miniconda/)

```bash
# 下载安装文件

# 校验miniconda是否为官方文件【可选】
sha256sum filename

# 安装miniconda
chmod +x filename
bash filename

# 配置环境变量
vim ~/.bashrc
export PATH=$PATH:$HOME/miniconda3/bin
source ~/.bashrc
```

## 权重下载

- 可通过HuggingFace与ModelScope进行模型权重下载，这里以modelscope为例

```bash
# 安装modelscope
pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple

# 下载相应模型
nohup modelscope download --model deepseek-ai/DeepSeek-R1 --local_dir /data01/ > ./download.log 2>&1 & 
```

## 拉取Vllm Docker镜像

Vllm凭借优异的推理性能，成为用户server部署的首选，通过vllm官网或云省分云盘下载后加载

```bash
# 将下载的vllm image上传至两台服务器
scp -p ./vllm.tar root@xx.xx.xx.xx:/root/

# 登录服务器加载镜像
docker load -i vllm.tar
```

## 模型部署

**模型部署分为单节点部署与多节点部署，其中多节点部署请将模型权重放到不同节点的相同路径下，并且确保不同节点的docker引擎版本、镜像版本一致**

### 单节点部署

- 运行vllm容器

```bash
# 运行容器
docker run -idt --entrypoint /bin/bash. \ 
    --network host \
    --privileged \
    --name vllm \
    --ipc host \ 
    --gpus all \
    -v /权重路径:/root/models
    容器镜像id

# 进入容器
docker exec -it vllm /bin/bash

# 部署openai api server
python3 -m vllm.entrypoints.openai.api_server 
--port 8000 
--model  /权重路径
--served-model-name QwQ-32B #模型名称 
--tensor-parallel-size 4 #显卡数量
--max-num-seqs 64 
--max_model_len=2048
--api-key=token-abc123 # api key

# 后台运行举例
nohup python3 -m vllm.entrypoints.openai.api_server --port 8000 --model /home/ubuntu/QwQ --served-model-name QwQ-32B --tensor-parallel-size 4 --max-num-seqs 64 --max_model_len=2048 --api-key=token-abc123 --gpu-memory-utilization=0.9 > ./ds_infer.log 2>&1 &
```

### 多节点部署

Vllm多节点部署是通过Ray实现，确保所有节点容器镜像一致，以deepseek 671B部署为例，具体部署步骤如下：

- **编辑Ray集群启动脚本**

```bash
#!/bin/bash

# Check for minimum number of required arguments
if [ $# -lt 4 ]; then
    echo "Usage: $0 docker_image head_node_address --head|--worker path_to_hf_home [additional_args...]"
    exit 1
fi

# Assign the first three arguments and shift them away
DOCKER_IMAGE="$1"
HEAD_NODE_ADDRESS="$2"
NODE_TYPE="$3"  # Should be --head or --worker
PATH_TO_MODEL_HOME="$4"
shift 4

# Additional arguments are passed directly to the Docker command
ADDITIONAL_ARGS=("$@")

# Validate node type
if [ "${NODE_TYPE}" != "--head" ] && [ "${NODE_TYPE}" != "--worker" ]; then
    echo "Error: Node type must be --head or --worker"
    exit 1
fi

# Define a function to cleanup on EXIT signal
cleanup() {
    docker stop node
    docker rm node
}
trap cleanup EXIT

# Command setup for head or worker node
RAY_START_CMD="ray start --block"
if [ "${NODE_TYPE}" == "--head" ]; then
    RAY_START_CMD+=" --head --port=6379"
else
    RAY_START_CMD+=" --address=${HEAD_NODE_ADDRESS}:6379"
fi

# Run the docker command with the user specified parameters and additional arguments
docker run \
    --entrypoint /bin/bash \
    --network host \
    --privileged \
    --name vllm \
    --ipc host \
    --gpus all \
    -v "${PATH_TO_MODEL_HOME}:/root/models" \
    "${ADDITIONAL_ARGS[@]}" \
    "${DOCKER_IMAGE}" -c "${RAY_START_CMD}"
```

- **不同Node节点启动脚本**

```bash
# Head节点
nohup bash run_cluster.sh a0b3e59739e9 192.168.0.2 --head /data01/ -e VLLM_HOST_IP=192.168.0.2 -e GLOO_SOCKET_IFNAME=eth0 -e NCCL_SOCKET_IFNAME=eth0 > ray.log 2>&1 &

# worker节点
nohup bash run_cluster.sh a0b3e59739e9 192.168.0.2 --worker /data01/ -e VLLM_HOST_IP=192.168.0.3 -e GLOO_SOCKET_IFNAME=eth0 -e NCCL_SOCKET_IFNAME=eth0 > ray.log 2>&1 &

# head节点查看ray集群状态
docker exec -it vllm /bin/bash
ray status # 观察节点是否active，以及所有节点纳入集群
```

- **拉起模型服务**

```bash
# Head节点运行
nohup python3 -m vllm.entrypoints.openai.api_server --port 8000 --model  /root/models/deepseek-r1/ --served-model-name DeepSeek-R1 --tensor-parallel-size 8 -pp 2 --max_model_len=2048 --api-key=token-abc123 --trust-remote-code --gpu-memory-utilization=0.9 > ./ds_infer.log 2>&1 &
# 查看状态
tail xx.log
```

## 模型服务测试

```bash
curl -H "Accept: application/json" -H "Content-type: application/json"  -X POST -d '{
"model": "DeepSeek-R1",
 "messages": [{"role": "user", "content": "介绍一下你自己"}],
 "stream": false,
 "parameters": {
  "temperature": 0.5,
  "top_k": 10,
  "top_p": 0.95,
  "max_new_tokens": 2048,
  "do_sample": true,
  "seed": null,
  "repetition_penalty": 1.03,
  "details": true,
  "typical_p": 0.5,
  "watermark": false,
  "stop": "<｜end▁of▁sentence｜>"
 }
}' http://117.89.85.158:1025/v1/chat/completions
```
